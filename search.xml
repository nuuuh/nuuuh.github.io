<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>DDPM</title>
    <url>/2023/08/21/DDPM/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h1><h2 id="OUTLINE"><a href="#OUTLINE" class="headerlink" title="OUTLINE"></a>OUTLINE</h2><ul>
<li>Overall</li>
<li>Forward Process (Diffusion Process)</li>
<li>Reverse Process (Denoise)</li>
<li>Optimization</li>
<li>Trainning and Sampling</li>
</ul>
<h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><p>A Diffusion Model, specifically Deep Diffusion Probabilistic Models (DDPM), is a type of generative model that employs a diffusion process to generate data samples.</p>
<p>Here’s a brief explanation of DDPM:</p>
<p>Diffusion Process: DDPM uses a diffusion process where, initially, generated samples start as noisy versions, typically random. Over multiple steps, the strength of the noise is gradually reduced, making the samples closer to real data. Each step is modeled as a probability distribution, with parameters controlled by neural networks.</p>
<p>Deep Neural Networks: DDPM employs deep neural networks to model the probability distribution at each diffusion step. This allows it to learn complex data distributions and progressively reduce noise to make samples resemble training data.</p>
<p>Reverse Process: DDPM can be used not only for sample generation but also for estimating the probability density function of data. This makes it suitable for density estimation and sampling tasks like image generation, denoising, and super-resolution.</p>
<p>Training: Training DDPM involves estimating the parameters of probability distributions during the diffusion process to closely match the true distribution of the training data. Common training techniques include maximum likelihood estimation.</p>
<p>One significant advantage of DDPM is its ability to generate high-quality images and perform denoising tasks effectively. By iteratively improving sample quality, it can produce very realistic samples, particularly in the realm of image generation. However, compared to other generative models, DDPM’s training and generation processes can be more complex and time-consuming.</p>
<p>– Generated by ChatGPT 3.5</p>
<h2 id="Forward-Process-Diffusion-Process"><a href="#Forward-Process-Diffusion-Process" class="headerlink" title="Forward Process (Diffusion Process)"></a>Forward Process (Diffusion Process)</h2><p>The forward process is a <strong>markov chain</strong> containing discrete states from time step 0 to T. During difussion at time <em>t</em>, the original image, denoted as $x_0$, is added noise $\epsilon_t$:<br>$$<br>x_t &#x3D; a_t x_{t-1} + b_t\epsilon_t　(1)\<br>\epsilon_t \sim N(0,I)\<br>a_t, b_t \in (0,1)<br>$$<br>where $a_t$ and $b_t$ are used to control the deviation of $x_{t-1}$.</p>
<p>After unfolding the recursive formula (1), we get:<br>$$<br>x_t &#x3D; (a_t…a_1)x_0 + (a_t … a_2)b_1\epsilon_1+(a_t…a_3)b_2\epsilon_2　…　+a_tb_{t-1}\epsilon_{t-1}+b_t\epsilon_t \<br>&#x3D;(a_t…a_1)x_0+\sqrt{\sum_{i&#x3D;2}^{t}{(a_t … a_{i})^2b_{t-1}^2}+b_t^2}　\overline{\epsilon_t}　(\overline{\epsilon_t}\sim N(0,I))<br>$$</p>
<p>To make the formula more simple, constraint $a_t^2+b_t^2&#x3D;1$ is introduced:<br>$$<br>x_t &#x3D; (a_t…a_1)x_0+\sqrt{-(a_t…a_1)^2+(a_t…a_2)^2a_1^2+(a_t…a_2)^2b_1^2+\sum_{i&#x3D;3}^{t}{(a_t … a_{i})^2b_{t-1}^2}+b_t^2}　\overline{\epsilon_t}\<br>&#x3D; (a_t…a_1)x_0+\sqrt{-(a_t…a_1)^2+(a_t…a_3)^2a_2^2+\sum_{i&#x3D;3}^{t}{(a_t … a_{i})^2b_{t-1}^2}+b_t^2}　\overline{\epsilon_t}\<br>&#x3D; (a_t…a_1)x_0+\sqrt{-(a_t…a_1)^2+(a_t…a_3)^2a_2^2+(a_t…a_3)^2b_2^2+\sum_{i&#x3D;4}^{t}{(a_t … a_{i})^2b_{t-1}^2}+b_t^2}　\overline{\epsilon_t}\<br>&#x3D; (a_t…a_1)x_0+\sqrt{-(a_t…a_1)^2+(a_t…a_4)^2a_3^2+\sum_{i&#x3D;4}^{t}{(a_t … a_{i})^2b_{t-1}^2}+b_t^2}　\overline{\epsilon_t}\<br>…\<br>&#x3D;(a_t…a_1)x_0+\sqrt{-(a_t…a_1)^2+a_t^2+b_t^2}　\overline{\epsilon_t}\<br>&#x3D;(a_t…a_1)x_0+\sqrt{1-(a_t…a_1)^2}　\overline{\epsilon_t}\<br>$$</p>
<p>Denote $a_t^2 &#x3D;\alpha_t$ and $(a_t…a_1)^2 &#x3D; \prod^t_{i&#x3D;1}{\alpha_i} &#x3D; \overline{\alpha_t}$ , and then we have the new recursive formula and the unfolded formula that satisfy the constraint:<br>$$<br>x_t &#x3D; \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}　\epsilon_t　　(2)\<br>x_t &#x3D; \sqrt{\overline{\alpha_t}}x_0+\sqrt{1-\overline{\alpha_t}}　\overline{\epsilon_t}　　(3)<br>$$</p>
<p>Since $\epsilon_t$ is a Gaussian distribution, we can write the forward transition probability of the markov chain as follow:<br>$$<br>x_t \sim q(x_t|x_{t-1}) &#x3D; N(x_t;\sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I)　（4）\<br>$$</p>
<p>And formula (3) derives:<br>$$<br>x_t \sim q(x_t|x_0) &#x3D; N(x_t;\sqrt{\overline{\alpha_t}}x_0, (1-\overline{\alpha_t})I)　(5)\<br>$$</p>
<p>The joint distribution of the forward process:<br>$$<br>q(x_{1:t}|x_0) &#x3D; \prod^T_{t&#x3D;1}{q(x_t|x_{t-1})}　(6)<br>$$</p>
<h2 id="Reverse-Process-Denoise"><a href="#Reverse-Process-Denoise" class="headerlink" title="Reverse Process (Denoise)"></a>Reverse Process (Denoise)</h2><p>The goal of the reverse process is to generate new samples within training distribution step by step from a Gaussian noise.</p>
<p>$p_\theta(x_{0:T})$ is called the reverse process, and it is defined as a Markov chain with learned Gaussian transitions starting at $p(x_T) &#x3D; N(x_T;0,I)$:<br>$$<br>p_\theta(x_{0:T}) &#x3D; p(x_T)\prod_{t&#x3D;1}^T{p_\theta(x_{t}|x_t+1)}　(7)\<br>p_\theta(x_{t-1}|x_t) &#x3D; N(x_{t-1}; \mu_\theta(x_t,t),\Sigma_\theta(x_t,t))　(8)\<br>$$<br>In DDPM, $\Sigma_\theta(x_t,t)$ are set to time depandent constants $\sigma_t^2$.</p>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>The target of training is to make the distribution generated by the model $p_\theta(x_0)$ similar to the distribution of the original data $q(x_0)$. This can be done by minimizing the cross entropy of the two distributions $L_{ce}&#x3D;-E_{q(x_0)}[log(p_\theta(x_0))]$　(also called maximizing the negative log-likelyhood).</p>
<blockquote>
<p>Why not using KL divergence?</p>
<p> KL divergence &#x3D; the entropy of p(x) + cross entropyy;</p>
<p>The distribution of real-world data is fixed so we only need to focus on cross entropy.</p>
</blockquote>
<p>$L_{ce}$ is difficult to compute directly, so an upper bound of $L_{ce}$ is calulated using Jensen’s Inequality:<br>$$<br>L_{ce} &#x3D; -E_{q(x_0)}[log(p_\theta(x_0))] 　(9)\<br>&#x3D;-E_q[log(\int p_\theta(x_{0:T})dx_{1:T})] \<br>&#x3D;-E_q[log(\int q(x_{1:T}|x_0)\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}dx_{1:T})] \<br>&#x3D;-E_q[log(E_{q(x_{1:T}|x_0)}(\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}))] \<br>\leq -E_q[E_{q(x_{1:T}|x_0)}(log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)})] \<br>&#x3D; -\iint q(x_0) q(x_{1:T}|x_0)log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}dx_0dx_{1:T} \<br>&#x3D; -\int q(x_{0:T})log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}dx_{0:T} \<br>&#x3D; E_{q(x_{0:T})}[-log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}]　（10）\<br>$$</p>
<p>Eq.10 can be further expressed using KL divergence and entropy:<br>$$<br>L &#x3D; E_{q(x_{0:T})}[-log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}] \<br>&#x3D; E_q[-log(p(x_T)) - \sum^T_{t&#x3D;1} log\frac{p_\theta(x_{t-1}|x_t)}{q(x_{t}|x_{t-1})}] \<br>&#x3D; E_q[-log(p(x_T)) - \sum^T_{t&#x3D;2} log\frac{p_\theta(x_{t-1}|x_t)}{q(x_{t}|x_{t-1})} - log\frac{p_\theta(x_0|x_1)}{q(x_1|x_0)}] \<br>&#x3D; E_q[-log(p(x_T)) - \sum^T_{t&#x3D;2} log\frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_{t},x_0)} \frac{q(x_{t-1}|x_0)}{q(x_t|x_0)} - log\frac{p_\theta(x_0|x_1)}{q(x_1|x_0)}] \<br>&#x3D; E_q[-log(p(x_T)) - \sum^T_{t&#x3D;2} log\frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_{t},x_0)} - log\frac{q(x_1|x_0)}{q(x_T|x_0)} - log\frac{p_\theta(x_0|x_1)}{q(x_1|x_0)}] \<br>&#x3D; E_q[-log(p(x_T)) - \sum^T_{t&#x3D;2} log\frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_{t},x_0)} - log\frac{q(x_1|x_0)}{q(x_T|x_0)} - log\frac{p_\theta(x_0|x_1)}{q(x_1|x_0)}] \<br>&#x3D; E_q[-log\frac{p(x_T)}{q(x_T|x_0)} - \sum^T_{t&#x3D;2} log\frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_{t},x_0)} - log(p_\theta(x_0|x_1))] \<br>&#x3D; E_q[D_{KL}(q(x_T|x_0)||p(x_T)) + \sum^T_{t&#x3D;2} D_{KL}(q(x_{t-1}|x_{t},x_0)||p_\theta(x_{t-1}|x_t)) - log(p_\theta(x_0|x_1))] 　(11)\<br>$$</p>
<p>Eventurally, the upper bound consists of three conponents. The first term in Eq.11 is called <em>prior matching term</em>, the second one is called <em>denoising matching term</em> and the last one is called <em>reconstruction term</em>.</p>
<p>To calculate the upper bound, we will need to characterize $q(x_{t-1}|x_t,x_0)$ and $p_\theta(x_{t-1}|x_t)$, which will help us solve denoising matching term. Prior matching term is already able to calculate and reconstruction term will be calculated using an independent model.</p>
<p><img src="/pics/DDPM/formula_p1.png" alt="formula"></p>
<p>Eventually, $q(x_{t-1}|x_t,x_0)$ is expressed as follow:<br><img src="/pics/DDPM/formula_12.png" alt="12"></p>
<p>According to the theorem below:<br>$$<br>D_{\mathrm{KL}}\left(\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{\mu}_x, \boldsymbol{\Sigma}_x\right) | \mathcal{N}\left(\boldsymbol{y} ; \boldsymbol{\mu}_y, \boldsymbol{\Sigma}_y\right)\right)&#x3D;\frac{1}{2}\left[\log \frac{\left|\boldsymbol{\Sigma}_y\right|}{\left|\boldsymbol{\Sigma}_x\right|}-d+\operatorname{tr}\left(\boldsymbol{\Sigma}_y^{-1} \boldsymbol{\Sigma}_x\right)+\left(\boldsymbol{\mu}_y-\boldsymbol{\mu}_x\right)^T \boldsymbol{\Sigma}_y^{-1}\left(\boldsymbol{\mu}_y-\boldsymbol{\mu}_x\right)\right]<br>$$</p>
<p>Assuming $p_\theta$ and $q$ have the same variance, then the denoising matching term can be expressed as follow:</p>
<p><img src="/pics/DDPM/formula_13.png" alt="13"></p>
<p>The only unknown variable in Eq.13 is $\mu_\theta$. DDPM parameterize $\mu_\theta$ using the same form of $\mu_q$ instead of directly predicting $\mu_\theta$, which gives:<br>$$<br>\mu_\theta &#x3D; \frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t} \hat x_0 + \frac{\sqrt{\alpha_{t}}(1-\overline\alpha_{t-1})}{1-\overline\alpha_t} x_t \<br>&#x3D; \frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t} \frac{x_t - \sqrt{1-\overline\alpha_t}\hat\epsilon_t}{\sqrt{\overline\alpha_t}} + \frac{\sqrt{\alpha_{t}}(1-\overline\alpha_{t-1})}{1-\overline\alpha_t} x_t \<br>&#x3D; \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1-\alpha_t}{\sqrt{1-\overline\alpha_t} \sqrt{\alpha_t}} \hat\epsilon_t　(14)\<br>$$</p>
<p>Therefore, to estimate $\mu_\theta$, we only need to predict the noise $\hat\epsilon_t&#x3D;f_\theta(x_t,t)$. Eq.13 is then transformed into:</p>
<p>$$<br>\begin{aligned} &amp; \underset{\boldsymbol{\theta}}{\arg\min} D_{\mathrm{KL}}(\left(q(x_{t-1} | x_t, x_0))\right | p_{\theta} (x_{t-1} | x_t)) &#x3D; \underset{\theta}{\arg \min } \frac{1}{2 \sigma_q^2(t)} \frac{(1-\alpha_t)^2}{(1-\bar{\alpha}_t)\alpha_t}[|f_\theta (x_t, t) - \varepsilon_t |_2^2]  \end{aligned} \tag{15}<br>$$</p>
<p>The final loss function $L$ can be expressed as below:<br>$$<br>L_{simple}(\theta) &#x3D; E_{t, x_0, \epsilon}[|\epsilon - \epsilon_\theta(\sqrt{\overline\alpha_t}x_0 + \sqrt{1-\overline\alpha_t}\epsilon, t)|_2^2]　（16）<br>$$</p>
<p>By comparing Eq.15 and Eq.16, we will find that $L$ is actually simplified, discarding the weights. As explained in DDPM, the simplified objective down-weights loss terms corresponding to small t. These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can<br>focus on more difficult denoising tasks at larger t terms.</p>
<h2 id="Training-and-Sampling"><a href="#Training-and-Sampling" class="headerlink" title="Training and Sampling"></a>Training and Sampling</h2><p><img src="/pics/DDPM/training_sampling.png" alt="Training and Sampling"><br><img src="/pics/DDPM/receiving.png" alt="Sending and Receiving"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Jonathan Ho, Ajay Jain, Pieter Abbeel; Denoising Diffusion Probabilistic Models, arXiv:2006.11239, 2020</p>
<p>[2] <a href="https://kxz18.github.io/2022/06/19/Diffusion/">https://kxz18.github.io/2022/06/19/Diffusion/</a></p>
<p>[3] <a href="https://zhuanlan.zhihu.com/p/565901160">https://zhuanlan.zhihu.com/p/565901160</a></p>
]]></content>
      <categories>
        <category>mathematics, computer vision, models</category>
      </categories>
      <tags>
        <tag>DDPM</tag>
        <tag>Generative model</tag>
      </tags>
  </entry>
  <entry>
    <title>Kalman Filtering</title>
    <url>/2023/08/16/Kalman-Filtering/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Kamlman-Filtering"><a href="#Kamlman-Filtering" class="headerlink" title="Kamlman Filtering"></a>Kamlman Filtering</h2>]]></content>
      <categories>
        <category>mathematics</category>
      </categories>
      <tags>
        <tag>Kalman Fitering</tag>
        <tag>Object Tracking</tag>
        <tag>Anti-noise</tag>
      </tags>
  </entry>
  <entry>
    <title>OptimalTransport</title>
    <url>/2023/07/27/OptimalTransport/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="Optimal-Transport-OT"><a href="#Optimal-Transport-OT" class="headerlink" title="Optimal Transport (OT)"></a>Optimal Transport (OT)</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2>]]></content>
      <categories>
        <category>Statistics</category>
      </categories>
      <tags>
        <tag>Math Tools</tag>
        <tag>Statistics</tag>
        <tag>Optimal Transport</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown Basics</title>
    <url>/2023/07/26/MarkdownBasics/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="Markdown-cheat-sheet"><a href="#Markdown-cheat-sheet" class="headerlink" title="Markdown cheat sheet"></a>Markdown cheat sheet</h1><h2 id="Text-Formatting"><a href="#Text-Formatting" class="headerlink" title="Text Formatting"></a>Text Formatting</h2><p>This is how to do <strong>bold</strong> text.<br>This is how to do <em>italic</em> text.<br>You can also do <strong>bold</strong> like this.<br>You can also do <em>italic</em> like this.</p>
<blockquote>
<p>This is how to make a blockquote</p>
</blockquote>
<h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><p>For a full description of the markdown syntax, visit <a href="http://daringfireball.net/projects/markdown/syntax">this page</a></p>
<h2 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h2><p>Unordered lists:</p>
<ul>
<li>item 1</li>
</ul>
<ul>
<li>item 2</li>
</ul>
<ul>
<li>item 3</li>
</ul>
<p>Ordered lists:</p>
<ol>
<li>hello</li>
<li>world</li>
</ol>
<p>Nested lists</p>
<ul>
<li>a<ul>
<li>b</li>
<li>c</li>
</ul>
</li>
<li>d</li>
</ul>
<h2 id="Python-Code"><a href="#Python-Code" class="headerlink" title=" Python Code "></a><mark> Python Code <mark></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hi!&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="C-Code"><a href="#C-Code" class="headerlink" title="C Code"></a>C Code</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="Bash-Code"><a href="#Bash-Code" class="headerlink" title="Bash Code"></a>Bash Code</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="This-is-an-H1"><a href="#This-is-an-H1" class="headerlink" title="This is an H1"></a>This is an H1</h1><p>here’s a quote block:</p>
<blockquote>
<p>quoting…</p>
</blockquote>
<h2 id="This-is-an-H2"><a href="#This-is-an-H2" class="headerlink" title="This is an H2"></a>This is an H2</h2><p>Switching to another row only<br>requires an emplty row in between or adding two spaces in the end.</p>
<h3 id="This-is-an-H3"><a href="#This-is-an-H3" class="headerlink" title="This is an H3"></a>This is an H3</h3><p>&emsp;An indentation requires “&amp;emsp;”.</p>
<h4 id="This-is-an-H4"><a href="#This-is-an-H4" class="headerlink" title="This is an H4"></a>This is an H4</h4><p>Inserting images:</p>
<p><img src="/pics/test.jpg" alt="test"></p>
<h5 id="This-is-an-H5"><a href="#This-is-an-H5" class="headerlink" title="This is an H5"></a>This is an H5</h5><p>Adding footnot requires<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup><br>Inline footnote<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>  </p>
<h6 id="This-is-an-H6"><a href="#This-is-an-H6" class="headerlink" title="This is an H6"></a>This is an H6</h6><p>This is a test for width………………………………………………………………………………………………………………………………………………………………………………</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">test1</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">test2</span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>condition DDPM</title>
    <url>/2023/08/24/condition-DDPM/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css">]]></content>
  </entry>
</search>
